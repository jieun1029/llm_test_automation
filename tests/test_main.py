"""
pytest ë©”ì¸ í…ŒìŠ¤íŠ¸ íŒŒì¼ (10ê°œ ì§ˆë¬¸ Ã— 2íšŒ ë°˜ë³µ, ëœë¤ ë”œë ˆì´)
"""
import pytest
import json
import time
import random
from datetime import datetime
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.session_manager import SessionManager
from core.api_client import APIClient
from core.evaluator import Evaluator
from reports.report_generator import generate_excel_report, save_detailed_answers_csv
from utils.logger import setup_logger, log_test_start, log_test_result, log_test_error
from utils.scoring import calculate_statistics, print_statistics, calculate_category_statistics, print_category_statistics

# ì„¤ì •
BASE_API_V1 = "https://api.myalan.ai/api/v1"
BASE_API_V2 = "https://api.myalan.ai/api/v2"
PERSONA_ID = "67a8266697ac2b9de6c51edf"
COOKIE_FILE = "cookies.json"

# ì „ì—­ ë³€ìˆ˜ (í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ìš©)
all_results = []


class TestLLMEvaluation:
    """LLM ë‹µë³€ í‰ê°€ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    @classmethod
    def setup_class(cls):
        """í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ì´ˆê¸°í™”"""
        print("\n" + "="*80)
        print("ğŸ¤– LLM ë‹µë³€ í‰ê°€ ìë™í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*80)
        
        # ë¡œê±° ì„¤ì •
        cls.logger = setup_logger()
        cls.logger.info("í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ì„¤ì • ë¡œë“œ
        with open("config/test_questions.json", "r", encoding="utf-8") as f:
            cls.test_data = json.load(f)
        
        cls.settings = cls.test_data["settings"]
        cls.test_cases = cls.test_data["test_cases"]
        cls.repeat_count = cls.settings["repeat_count"]
        cls.delay_between_tests = cls.settings["delay_between_tests"]
        cls.delay_between_rounds = cls.settings.get("delay_between_rounds", 5)
        
        cls.logger.info(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜: {len(cls.test_cases)}ê°œ")
        cls.logger.info(f"ë°˜ë³µ íšŸìˆ˜: {cls.repeat_count}íšŒ")
        
        # ì˜ˆìƒ ì†Œìš” ì‹œê°„ ê³„ì‚°
        if isinstance(cls.delay_between_tests, list):
            avg_delay = sum(cls.delay_between_tests) / 2
        else:
            avg_delay = cls.delay_between_tests
        
        total_time = (len(cls.test_cases) - 1) * avg_delay * cls.repeat_count + cls.delay_between_rounds * (cls.repeat_count - 1)
        cls.logger.info(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {int(total_time / 60)}ë¶„")
        
        # ì„¸ì…˜ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        cls.session_manager = SessionManager(COOKIE_FILE)
        cls.session_manager.load_session()
        
        # ë¡œê·¸ì¸ í™•ì¸
        success, user_name = cls.session_manager.check_login(BASE_API_V2)
        if not success:
            cls.logger.error("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨. cookies.jsonì„ í™•ì¸í•˜ì„¸ìš”.")
            pytest.exit("ë¡œê·¸ì¸ ì‹¤íŒ¨")
        
        cls.logger.info(f"âœ… ë¡œê·¸ì¸ ì„±ê³µ: {user_name} (ID: {cls.session_manager.get_user_id()})")
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        cls.api_client = APIClient(
            session=cls.session_manager.session,
            base_api_v1=BASE_API_V1,
            base_api_v2=BASE_API_V2,
            persona_id=PERSONA_ID,
            user_id=cls.session_manager.get_user_id()
        )
        
        # í‰ê°€ì ì´ˆê¸°í™”
        cls.evaluator = Evaluator("config/evaluation_criteria.json")
        
        cls.logger.info("âœ… í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\n")
    
    def test_llm_response(self, test_params):
        """
        LLM ë‹µë³€ í‰ê°€ í…ŒìŠ¤íŠ¸ (ë™ì  íŒŒë¼ë¯¸í„°)
        
        Args:
            test_params: pytest fixture (round_num, test_case í¬í•¨)
        """
        # íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        round_num = test_params['round_num']
        test_case = test_params['test_case']
        
        # ë¡œê·¸ ì‹œì‘
        log_test_start(self.logger, test_case['id'], test_case['question'], round_num)
        
        # 1. ì±„ë„ ìƒì„± ë° ì§ˆë¬¸ ì „ì†¡
        success, answer, error_msg = self.api_client.execute_test(test_case['question'])
        
        if not success:
            log_test_error(self.logger, test_case['id'], error_msg)
            pytest.fail(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {error_msg}")
        
        self.logger.info(f"ğŸ’¬ ë‹µë³€ ìˆ˜ì‹  ì™„ë£Œ ({len(answer)}ì)")
        
        # 2. ë‹µë³€ í‰ê°€
        evaluation = self.evaluator.evaluate_answer(test_case, answer)
        
        # ë¡œê·¸ ê²°ê³¼
        log_test_result(self.logger, test_case['id'], evaluation, len(answer))
        
        # 3. ê²°ê³¼ ì €ì¥
        result = {
            "test_id": test_case['id'],
            "round": round_num,
            "question": test_case['question'],
            "category": test_case['category'],
            "answer": answer,
            "evaluation": evaluation,
            "timestamp": datetime.now().isoformat()
        }
        
        all_results.append(result)
        
        # 4. ëœë¤ ëŒ€ê¸° (ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „)
        if self.delay_between_tests:
            # ë¦¬ìŠ¤íŠ¸ë©´ ëœë¤, ìˆ«ìë©´ ê·¸ëŒ€ë¡œ
            if isinstance(self.delay_between_tests, list):
                delay = random.randint(self.delay_between_tests[0], self.delay_between_tests[1])
            else:
                delay = self.delay_between_tests
            
            self.logger.info(f"â³ ë‹¤ìŒ í…ŒìŠ¤íŠ¸ê¹Œì§€ {delay}ì´ˆ ëŒ€ê¸°...\n")
            time.sleep(delay)
        
        # Assertion (pytest fail ì²˜ë¦¬)
        assert evaluation['pass'], f"í‰ê°€ ì‹¤íŒ¨: ì´ì  {evaluation['total_score']}/{evaluation['max_score']}ì "
    
    @classmethod
    def teardown_class(cls):
        """í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ ì¢…ë£Œ ì²˜ë¦¬"""
        if not all_results:
            cls.logger.warning("âš ï¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        cls.logger.info("\n" + "="*80)
        cls.logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ê²°ê³¼ ì €ì¥ ì¤‘...")
        cls.logger.info("="*80)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ í´ë” ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = f"output/{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        
        # Excel ë¦¬í¬íŠ¸ ìƒì„±
        generate_excel_report(all_results, f"{output_dir}/test_results.xlsx")
        cls.logger.info(f"âœ… Excel ë¦¬í¬íŠ¸ ìƒì„±: {output_dir}/test_results.xlsx")
        
        # ìƒì„¸ ë‹µë³€ CSV ì €ì¥
        save_detailed_answers_csv(all_results, f"{output_dir}/detailed_answers.csv")
        cls.logger.info(f"âœ… ìƒì„¸ ë‹µë³€ CSV ì €ì¥: {output_dir}/detailed_answers.csv")
        
        # í†µê³„ ì¶œë ¥
        stats = calculate_statistics(all_results)
        print_statistics(stats)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = calculate_category_statistics(all_results)
        print_category_statistics(category_stats)
        
        cls.logger.info("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")


def pytest_configure(config):
    """pytest ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•"""
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
    with open("config/test_questions.json", "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    config.test_cases = test_data["test_cases"]
    config.addinivalue_line("markers", "round: í…ŒìŠ¤íŠ¸ ë¼ìš´ë“œ ë§ˆì»¤")


def pytest_generate_tests(metafunc):
    """ë™ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ìƒì„±"""
    if "test_params" in metafunc.fixturenames:
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        import json
        with open("config/test_questions.json", "r", encoding="utf-8") as f:
            test_data = json.load(f)
        
        repeat_count = test_data["settings"]["repeat_count"]
        test_cases = test_data["test_cases"]
        
        # íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
        params = []
        for round_num in range(1, repeat_count + 1):
            for test_case in test_cases:
                params.append({
                    'round_num': round_num,
                    'test_case': test_case
                })
        
        # í…ŒìŠ¤íŠ¸ ID ìƒì„±
        ids = [f"Round{p['round_num']}-{p['test_case']['id']}" for p in params]
        
        metafunc.parametrize("test_params", params, ids=ids)


@pytest.fixture
def test_params(request):
    """í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° fixture"""
    return request.param


def pytest_collection_modifyitems(config, items):
    """í…ŒìŠ¤íŠ¸ ì•„ì´í…œ ìˆ˜ì • (ì •ë ¬ ë“±)"""
    # ë¼ìš´ë“œ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    items.sort(key=lambda item: item.nodeid)
